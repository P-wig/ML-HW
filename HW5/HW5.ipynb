{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 5\n",
    "\n",
    "This assignment covers Comparision of Decision Trees and Support Vector Machine. \n",
    "**DO NOT ERASE MARKDOWN CELLS AND INSTRUCTIONS IN YOUR HW submission**\n",
    "\n",
    "  * **Q** - QUESTION\n",
    "  * **A** - Where to input your answer\n",
    "\n",
    "## Instructions\n",
    "\n",
    "Keep the following in mind for all notebooks you develop:\n",
    "* Structure your notebook. \n",
    "* Use headings with meaningful levels in Markdown cells, and explain the questions each piece of code is to answer or the reason it is there.\n",
    "* Make sure your notebook can always be rerun from top to bottom.\n",
    "* Objective of this assignment is to help you master python and scikit-learn package. \n",
    "* See [README.md](README.md) for homework submission instructions\n",
    "\n",
    "## Related Tutorials\n",
    "    \n",
    "* [Decision Tree with KFold Cross Validation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)\n",
    "\n",
    "* [Decision Tree with Bagging](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html#sklearn.ensemble.BaggingRegressor)\n",
    "\n",
    "* [Support Vector Machine](https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1** Get training data from the dataframe\n",
    "1. Load HW5_data.csv from ```data'' folder into the dataframe\n",
    "2. Check if there is any NaN in the dataset\n",
    "3. Remove the rows with NaN values.\n",
    "4. Print how many examples belong to each class in the data frame.\n",
    "\n",
    "**A1** Replace ??? with code in the code cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean of the integrated profile</th>\n",
       "      <th>Standard deviation of the integrated profile</th>\n",
       "      <th>Excess kurtosis of the integrated profile</th>\n",
       "      <th>Skewness of the integrated profile</th>\n",
       "      <th>Mean of the DM-SNR curve</th>\n",
       "      <th>Standard deviation of the DM-SNR curve</th>\n",
       "      <th>Excess kurtosis of the DM-SNR curve</th>\n",
       "      <th>Skewness of the DM-SNR curve</th>\n",
       "      <th>target_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>121.156250</td>\n",
       "      <td>48.372971</td>\n",
       "      <td>0.375485</td>\n",
       "      <td>-0.013165</td>\n",
       "      <td>3.168896</td>\n",
       "      <td>18.399367</td>\n",
       "      <td>7.449874</td>\n",
       "      <td>65.159298</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>76.968750</td>\n",
       "      <td>36.175557</td>\n",
       "      <td>0.712898</td>\n",
       "      <td>3.388719</td>\n",
       "      <td>2.399666</td>\n",
       "      <td>17.570997</td>\n",
       "      <td>9.414652</td>\n",
       "      <td>102.722975</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>130.585938</td>\n",
       "      <td>53.229534</td>\n",
       "      <td>0.133408</td>\n",
       "      <td>-0.297242</td>\n",
       "      <td>2.743311</td>\n",
       "      <td>22.362553</td>\n",
       "      <td>8.508364</td>\n",
       "      <td>74.031324</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>156.398438</td>\n",
       "      <td>48.865942</td>\n",
       "      <td>-0.215989</td>\n",
       "      <td>-0.171294</td>\n",
       "      <td>17.471572</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.958066</td>\n",
       "      <td>7.197842</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84.804688</td>\n",
       "      <td>36.117659</td>\n",
       "      <td>0.825013</td>\n",
       "      <td>3.274125</td>\n",
       "      <td>2.790134</td>\n",
       "      <td>20.618009</td>\n",
       "      <td>8.405008</td>\n",
       "      <td>76.291128</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>121.007812</td>\n",
       "      <td>47.176944</td>\n",
       "      <td>0.229708</td>\n",
       "      <td>0.091336</td>\n",
       "      <td>2.036789</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.546051</td>\n",
       "      <td>112.131721</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>79.343750</td>\n",
       "      <td>42.402174</td>\n",
       "      <td>1.063413</td>\n",
       "      <td>2.244377</td>\n",
       "      <td>141.641304</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.700809</td>\n",
       "      <td>-1.200653</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>109.406250</td>\n",
       "      <td>55.912521</td>\n",
       "      <td>0.565106</td>\n",
       "      <td>0.056247</td>\n",
       "      <td>2.797659</td>\n",
       "      <td>19.496527</td>\n",
       "      <td>9.443282</td>\n",
       "      <td>97.374578</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>95.007812</td>\n",
       "      <td>40.219805</td>\n",
       "      <td>0.347578</td>\n",
       "      <td>1.153164</td>\n",
       "      <td>2.770067</td>\n",
       "      <td>18.217741</td>\n",
       "      <td>7.851205</td>\n",
       "      <td>70.801938</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>109.156250</td>\n",
       "      <td>47.002234</td>\n",
       "      <td>0.394182</td>\n",
       "      <td>0.190296</td>\n",
       "      <td>4.578595</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.702532</td>\n",
       "      <td>36.342493</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Mean of the integrated profile  \\\n",
       "0                       121.156250   \n",
       "1                        76.968750   \n",
       "2                       130.585938   \n",
       "3                       156.398438   \n",
       "4                        84.804688   \n",
       "5                       121.007812   \n",
       "6                        79.343750   \n",
       "7                       109.406250   \n",
       "8                        95.007812   \n",
       "9                       109.156250   \n",
       "\n",
       "    Standard deviation of the integrated profile  \\\n",
       "0                                      48.372971   \n",
       "1                                      36.175557   \n",
       "2                                      53.229534   \n",
       "3                                      48.865942   \n",
       "4                                      36.117659   \n",
       "5                                      47.176944   \n",
       "6                                      42.402174   \n",
       "7                                      55.912521   \n",
       "8                                      40.219805   \n",
       "9                                      47.002234   \n",
       "\n",
       "    Excess kurtosis of the integrated profile  \\\n",
       "0                                    0.375485   \n",
       "1                                    0.712898   \n",
       "2                                    0.133408   \n",
       "3                                   -0.215989   \n",
       "4                                    0.825013   \n",
       "5                                    0.229708   \n",
       "6                                    1.063413   \n",
       "7                                    0.565106   \n",
       "8                                    0.347578   \n",
       "9                                    0.394182   \n",
       "\n",
       "    Skewness of the integrated profile   Mean of the DM-SNR curve  \\\n",
       "0                            -0.013165                   3.168896   \n",
       "1                             3.388719                   2.399666   \n",
       "2                            -0.297242                   2.743311   \n",
       "3                            -0.171294                  17.471572   \n",
       "4                             3.274125                   2.790134   \n",
       "5                             0.091336                   2.036789   \n",
       "6                             2.244377                 141.641304   \n",
       "7                             0.056247                   2.797659   \n",
       "8                             1.153164                   2.770067   \n",
       "9                             0.190296                   4.578595   \n",
       "\n",
       "    Standard deviation of the DM-SNR curve  \\\n",
       "0                                18.399367   \n",
       "1                                17.570997   \n",
       "2                                22.362553   \n",
       "3                                      NaN   \n",
       "4                                20.618009   \n",
       "5                                      NaN   \n",
       "6                                      NaN   \n",
       "7                                19.496527   \n",
       "8                                18.217741   \n",
       "9                                      NaN   \n",
       "\n",
       "    Excess kurtosis of the DM-SNR curve   Skewness of the DM-SNR curve  \\\n",
       "0                              7.449874                      65.159298   \n",
       "1                              9.414652                     102.722975   \n",
       "2                              8.508364                      74.031324   \n",
       "3                              2.958066                       7.197842   \n",
       "4                              8.405008                      76.291128   \n",
       "5                              9.546051                     112.131721   \n",
       "6                             -0.700809                      -1.200653   \n",
       "7                              9.443282                      97.374578   \n",
       "8                              7.851205                      70.801938   \n",
       "9                              5.702532                      36.342493   \n",
       "\n",
       "   target_class  \n",
       "0           0.0  \n",
       "1           0.0  \n",
       "2           0.0  \n",
       "3           0.0  \n",
       "4           0.0  \n",
       "5           0.0  \n",
       "6           0.0  \n",
       "7           0.0  \n",
       "8           0.0  \n",
       "9           0.0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Read the data file using the propriate separator as input to read_csv()\n",
    "\n",
    "df = pd.read_csv('../data/HW5_data.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " Mean of the integrated profile                     0\n",
       " Standard deviation of the integrated profile       0\n",
       " Excess kurtosis of the integrated profile       1735\n",
       " Skewness of the integrated profile                 0\n",
       " Mean of the DM-SNR curve                           0\n",
       " Standard deviation of the DM-SNR curve          1178\n",
       " Excess kurtosis of the DM-SNR curve                0\n",
       " Skewness of the DM-SNR curve                     625\n",
       "target_class                                        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if there is NaN in the dataset\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target_class\n",
       "0.0    11375\n",
       "1.0     1153\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Drop NaNs if there is any\n",
    "df.dropna()\n",
    "\n",
    "# Count number of entries for different target_class\n",
    "df['target_class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2** Separate training and testing data from the dataframe\n",
    "\n",
    "1. Assign values of ```target_class``` column to ```y```, note you have to use ```.values``` method\n",
    "2. Drop ```target_class``` column from data frame,\n",
    "3. Assign df values to x\n",
    "4. Split dataset into train and test data use train_test_split with test_size = 0.25, stratify y and random_state = 1238\n",
    "\n",
    "**A2** Replace ??? with code in the code cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x: (12528, 8)\n",
      "Shape of y: (12528,)\n"
     ]
    }
   ],
   "source": [
    "# Assign values of ```target_class``` column to y, note you have to use .values method\n",
    "y = df['target_class'].values\n",
    "# Drop 'target_class' column from data frame,\n",
    "df.drop( columns=['target_class'], inplace=True )\n",
    "# Assign df values to x\n",
    "x = df.values\n",
    "# View shape of x and y\n",
    "print( \"Shape of x:\", x.shape )\n",
    "print( \"Shape of y:\", y.shape )\n",
    "\n",
    "xtrain, xtest, ytrain, ytest =  train_test_split( x, y, test_size=0.25, stratify=y, random_state=1238 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NsUM8-i_KhK1"
   },
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree with different depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3** Train DecisionTreeClassifier Model at different depths \n",
    "1. Create four [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) models with different parameters. Use max_depth size = 1, 2, 5, 25 & max_leaf_nodes=5, 10, 15, 25 respectively\n",
    "2. Use random_state=30 & criterion='entropy' for all models\n",
    "3. Fit the four different models with the train data.\n",
    "4. Predict the test data using trained models \n",
    "5. Calculate the Mean Squared Error(MSE) of each model's prediction\n",
    "6. Print precision recall curve for the test data with the minimum MSE value from four trianed models.\n",
    "\n",
    "**A3** Replace ??? with code in the code cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum MSE  0.02681992337164751\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#create decision tree classifier\n",
    "clf_1 = DecisionTreeClassifier( max_depth=1, max_leaf_nodes=5, random_state=30, criterion='entropy' )\n",
    "clf_2 = DecisionTreeClassifier( max_depth=2, max_leaf_nodes=10, random_state=30, criterion='entropy' )\n",
    "clf_3 = DecisionTreeClassifier( max_depth=5, max_leaf_nodes=15, random_state=30, criterion='entropy' )\n",
    "clf_4 = DecisionTreeClassifier( max_depth=25, max_leaf_nodes=25, random_state=30, criterion='entropy' )\n",
    "\n",
    "#fit classifier model\n",
    "clf_1.fit( xtrain, ytrain )\n",
    "clf_2.fit( xtrain, ytrain )\n",
    "clf_3.fit( xtrain, ytrain )\n",
    "clf_4.fit( xtrain, ytrain )\n",
    "\n",
    "#predict\n",
    "pred_1 = clf_1.predict( xtest )\n",
    "pred_2 = clf_2.predict( xtest )\n",
    "pred_3 = clf_3.predict( xtest )\n",
    "pred_4 = clf_4.predict( xtest )\n",
    "\n",
    "#calculate mean_squared_error\n",
    "mse_1 = mean_squared_error( ytest, pred_1 )\n",
    "mse_2 = mean_squared_error( ytest, pred_2 )\n",
    "mse_3 = mean_squared_error( ytest, pred_3 )\n",
    "mse_4 = mean_squared_error( ytest, pred_4 )\n",
    "\n",
    "print( \"minimum MSE\", min( mse_1, mse_2, mse_3, mse_4 ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision-Recall Curve for Best Above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Important Note:`` If ``from_estimator()`` function gives Attribute error then it means your sklearn is not updated.\n",
    "\n",
    "* If you are using conda, you can upgrade with\n",
    "\n",
    "conda upgrade -c conda-forge scikit-learn\n",
    "\n",
    "* or, with pip,\n",
    "\n",
    "python -m pip install scikit-learn --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IPython -- An enhanced Interactive Python\n",
      "=========================================\n",
      "\n",
      "IPython offers a fully compatible replacement for the standard Python\n",
      "interpreter, with convenient shell features, special commands, command\n",
      "history mechanism and output results caching.\n",
      "\n",
      "At your system command line, type 'ipython -h' to see the command line\n",
      "options available. This document only describes interactive features.\n",
      "\n",
      "GETTING HELP\n",
      "------------\n",
      "\n",
      "Within IPython you have various way to access help:\n",
      "\n",
      "  ?         -> Introduction and overview of IPython's features (this screen).\n",
      "  object?   -> Details about 'object'.\n",
      "  object??  -> More detailed, verbose information about 'object'.\n",
      "  %quickref -> Quick reference of all IPython specific syntax and magics.\n",
      "  help      -> Access Python's own help system.\n",
      "\n",
      "If you are in terminal IPython you can quit this screen by pressing `q`.\n",
      "\n",
      "\n",
      "MAIN FEATURES\n",
      "-------------\n",
      "\n",
      "* Access to the standard Python help with object docstrings and the Python\n",
      "  manuals. Simply type 'help' (no quotes) to invoke it.\n",
      "\n",
      "* Magic commands: type %magic for information on the magic subsystem.\n",
      "\n",
      "* System command aliases, via the %alias command or the configuration file(s).\n",
      "\n",
      "* Dynamic object information:\n",
      "\n",
      "  Typing ?word or word? prints detailed information about an object. Certain\n",
      "  long strings (code, etc.) get snipped in the center for brevity.\n",
      "\n",
      "  Typing ??word or word?? gives access to the full information without\n",
      "  snipping long strings. Strings that are longer than the screen are printed\n",
      "  through the less pager.\n",
      "\n",
      "  The ?/?? system gives access to the full source code for any object (if\n",
      "  available), shows function prototypes and other useful information.\n",
      "\n",
      "  If you just want to see an object's docstring, type '%pdoc object' (without\n",
      "  quotes, and without % if you have automagic on).\n",
      "\n",
      "* Tab completion in the local namespace:\n",
      "\n",
      "  At any time, hitting tab will complete any available python commands or\n",
      "  variable names, and show you a list of the possible completions if there's\n",
      "  no unambiguous one. It will also complete filenames in the current directory.\n",
      "\n",
      "* Search previous command history in multiple ways:\n",
      "\n",
      "  - Start typing, and then use arrow keys up/down or (Ctrl-p/Ctrl-n) to search\n",
      "    through the history items that match what you've typed so far.\n",
      "\n",
      "  - Hit Ctrl-r: opens a search prompt. Begin typing and the system searches\n",
      "    your history for lines that match what you've typed so far, completing as\n",
      "    much as it can.\n",
      "\n",
      "  - %hist: search history by index.\n",
      "\n",
      "* Persistent command history across sessions.\n",
      "\n",
      "* Logging of input with the ability to save and restore a working session.\n",
      "\n",
      "* System shell with !. Typing !ls will run 'ls' in the current directory.\n",
      "\n",
      "* The reload command does a 'deep' reload of a module: changes made to the\n",
      "  module since you imported will actually be available without having to exit.\n",
      "\n",
      "* Verbose and colored exception traceback printouts. See the magic xmode and\n",
      "  xcolor functions for details (just type %magic).\n",
      "\n",
      "* Input caching system:\n",
      "\n",
      "  IPython offers numbered prompts (In/Out) with input and output caching. All\n",
      "  input is saved and can be retrieved as variables (besides the usual arrow\n",
      "  key recall).\n",
      "\n",
      "  The following GLOBAL variables always exist (so don't overwrite them!):\n",
      "  _i: stores previous input.\n",
      "  _ii: next previous.\n",
      "  _iii: next-next previous.\n",
      "  _ih : a list of all input _ih[n] is the input from line n.\n",
      "\n",
      "  Additionally, global variables named _i<n> are dynamically created (<n>\n",
      "  being the prompt counter), such that _i<n> == _ih[<n>]\n",
      "\n",
      "  For example, what you typed at prompt 14 is available as _i14 and _ih[14].\n",
      "\n",
      "  You can create macros which contain multiple input lines from this history,\n",
      "  for later re-execution, with the %macro function.\n",
      "\n",
      "  The history function %hist allows you to see any part of your input history\n",
      "  by printing a range of the _i variables. Note that inputs which contain\n",
      "  magic functions (%) appear in the history with a prepended comment. This is\n",
      "  because they aren't really valid Python code, so you can't exec them.\n",
      "\n",
      "* Output caching system:\n",
      "\n",
      "  For output that is returned from actions, a system similar to the input\n",
      "  cache exists but using _ instead of _i. Only actions that produce a result\n",
      "  (NOT assignments, for example) are cached. If you are familiar with\n",
      "  Mathematica, IPython's _ variables behave exactly like Mathematica's %\n",
      "  variables.\n",
      "\n",
      "  The following GLOBAL variables always exist (so don't overwrite them!):\n",
      "  _ (one underscore): previous output.\n",
      "  __ (two underscores): next previous.\n",
      "  ___ (three underscores): next-next previous.\n",
      "\n",
      "  Global variables named _<n> are dynamically created (<n> being the prompt\n",
      "  counter), such that the result of output <n> is always available as _<n>.\n",
      "\n",
      "  Finally, a global dictionary named _oh exists with entries for all lines\n",
      "  which generated output.\n",
      "\n",
      "* Directory history:\n",
      "\n",
      "  Your history of visited directories is kept in the global list _dh, and the\n",
      "  magic %cd command can be used to go to any entry in that list.\n",
      "\n",
      "* Auto-parentheses and auto-quotes (adapted from Nathan Gray's LazyPython)\n",
      "\n",
      "  1. Auto-parentheses\n",
      "        \n",
      "     Callable objects (i.e. functions, methods, etc) can be invoked like\n",
      "     this (notice the commas between the arguments)::\n",
      "       \n",
      "         In [1]: callable_ob arg1, arg2, arg3\n",
      "       \n",
      "     and the input will be translated to this::\n",
      "       \n",
      "         callable_ob(arg1, arg2, arg3)\n",
      "       \n",
      "     This feature is off by default (in rare cases it can produce\n",
      "     undesirable side-effects), but you can activate it at the command-line\n",
      "     by starting IPython with `--autocall 1`, set it permanently in your\n",
      "     configuration file, or turn on at runtime with `%autocall 1`.\n",
      "\n",
      "     You can force auto-parentheses by using '/' as the first character\n",
      "     of a line.  For example::\n",
      "       \n",
      "          In [1]: /globals             # becomes 'globals()'\n",
      "       \n",
      "     Note that the '/' MUST be the first character on the line!  This\n",
      "     won't work::\n",
      "       \n",
      "          In [2]: print /globals    # syntax error\n",
      "\n",
      "     In most cases the automatic algorithm should work, so you should\n",
      "     rarely need to explicitly invoke /. One notable exception is if you\n",
      "     are trying to call a function with a list of tuples as arguments (the\n",
      "     parenthesis will confuse IPython)::\n",
      "       \n",
      "          In [1]: zip (1,2,3),(4,5,6)  # won't work\n",
      "       \n",
      "     but this will work::\n",
      "       \n",
      "          In [2]: /zip (1,2,3),(4,5,6)\n",
      "          ------> zip ((1,2,3),(4,5,6))\n",
      "          Out[2]= [(1, 4), (2, 5), (3, 6)]\n",
      "\n",
      "     IPython tells you that it has altered your command line by\n",
      "     displaying the new command line preceded by -->.  e.g.::\n",
      "       \n",
      "          In [18]: callable list\n",
      "          -------> callable (list)\n",
      "\n",
      "  2. Auto-Quoting\n",
      "    \n",
      "     You can force auto-quoting of a function's arguments by using ',' as\n",
      "     the first character of a line.  For example::\n",
      "       \n",
      "          In [1]: ,my_function /home/me   # becomes my_function(\"/home/me\")\n",
      "\n",
      "     If you use ';' instead, the whole argument is quoted as a single\n",
      "     string (while ',' splits on whitespace)::\n",
      "       \n",
      "          In [2]: ,my_function a b c   # becomes my_function(\"a\",\"b\",\"c\")\n",
      "          In [3]: ;my_function a b c   # becomes my_function(\"a b c\")\n",
      "\n",
      "     Note that the ',' MUST be the first character on the line!  This\n",
      "     won't work::\n",
      "       \n",
      "          In [4]: x = ,my_function /home/me    # syntax error\n"
     ]
    }
   ],
   "source": [
    "# Use the below one\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "# Or this below one, whichever suits you\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "?\n",
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree with K-fold cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4** Use Kfold on the test dataset, and evaluate the best model\n",
    "1. Use cross_val_score and fit your best model with k = 5 fold size on test data\n",
    "2. Calculate average scores in kfold\n",
    "\n",
    "**A4** Replace ??? with code in the code cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4150087386.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[26], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    scores = ?\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "scores = ?\n",
    "print(\"Cross-validation scores: {}\".format(?))\n",
    "print(\"Average cross-validation score: {:.2f}\".format(?))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Decision Tree with Bagging "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5** Now we will use Bagging technique on the our previous best model, and evaluate it\n",
    "\n",
    "```Part 1:```\n",
    "1. Now, Create a Bagged Model passing  ```model = previous_best, n_estimators = 10 & random_state=1 to BaggingClassifier()```\n",
    "2. Fit the model with the train data\n",
    "3. Predict the values with the test data\n",
    "4. Calculate the test MSE \n",
    "5. Plot Precision-Recall Curve from the true & predicted test data\n",
    "\n",
    "**A5** Replace ??? with code in the code cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Use BaggingRegressor to fit the training data\n",
    "# Calculate the mean squared error \n",
    "\n",
    "#load BaggingRegressor model and pass n_estimators=10, random_state=1\n",
    "bagged_clf = ?\n",
    "bagged_clf.?\n",
    "pred = ?\n",
    "?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pass necessary parameters to PrecisionRecallDisplay.from_estimator() \n",
    "\n",
    "PrecisionRecallDisplay.from_estimator(?)\n",
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Part 2: ```\n",
    "\n",
    "1. Why BaggingClassifier is called an ensembled technique? why it works better most of the time than the single model classifiers?\n",
    "\n",
    "#### Your answer goes here.  \n",
    "\n",
    "2. What is the disadvantage of incresing the number of estimators while using BaggingClassifier? Explain with an appropriate example.\n",
    "\n",
    "#### Your answer goes here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine(SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6** Create SVM Model on the training set, and do the following\n",
    "\n",
    "```Part:1```\n",
    "1. Now, Create a SVM Model with default parameters\n",
    "2. Fit the model with the train data\n",
    "3. Predict the values with the test data\n",
    "4. Calculate the model accuracy on test data\n",
    "5. Plot confusion matrix on the test data  (Make font size 16)\n",
    "\n",
    "\n",
    "**A6** Replace ??? with code in the code cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import SVC classifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "svc=?\n",
    "\n",
    "# fit classifier to training set\n",
    "?\n",
    "\n",
    "\n",
    "# make predictions on test set\n",
    "?\n",
    "\n",
    "\n",
    "# compute and print accuracy score\n",
    "print('Model accuracy score with default hyperparameters: {0:0.4f}'. format(?)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = ?\n",
    "\n",
    "plt.title(\"SVM Model - Confusion Matrix\")\n",
    "?\n",
    "?\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Part2:```\n",
    "\n",
    "1. From the above Confusion Matrix we can see that high number of Class 1 is predicted as Class 0 from the model. What is your reasoning behind this situtation? \n",
    "\n",
    "#### Your answer goes here.\n",
    "\n",
    "2. What can be done in order to resolve this issue?\n",
    "\n",
    "#### Your answer goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM with high margin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7** Create SVM Model on the training set, and evaluate\n",
    "\n",
    "``Note:``\n",
    "1. If we analyze our dataset using df.describe() function, we will see that there are many outliers in the dataset.\n",
    "2. So, we need to increase our margin with ```HIGH C``` values so that the SVM model get better generalization\n",
    "\n",
    "``Task:``\n",
    "\n",
    "1. Now, Create a SVM Model with rbf kernel and C=100\n",
    "2. Fit the model with the train data\n",
    "3. Predict the values with the test data\n",
    "4. Calculate the model accuracy on test data \n",
    "5. Plot Confusion Matrix from the true & predicted test data (Make font size 16)\n",
    "\n",
    "\n",
    "**A7** Replace ??? with code in the code cell below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate classifier with rbf kernel and C=100\n",
    "svc=?\n",
    "\n",
    "\n",
    "# fit classifier to training set\n",
    "?\n",
    "\n",
    "\n",
    "# make predictions on test set\n",
    "?\n",
    "\n",
    "\n",
    "# compute and print accuracy score\n",
    "print('Model accuracy score with rbf kernel and C=100.0 : {0:0.4f}'. format(?))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as pl\n",
    "\n",
    "cm = ?\n",
    "?\n",
    "?\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM with linear kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q8** Create SVM Model on the training set, and evaluate\n",
    "\n",
    "``Task:``\n",
    "\n",
    "1. Now, Create a SVM Model with linear kernel and C=1.0\n",
    "2. Fit the model with the train data\n",
    "3. Predict the values with the test data\n",
    "4. Calculate the model accuracy on test data\n",
    "5. Plot Confusion Matrix from the true & predicted test data (Make font size 16)\n",
    "\n",
    "\n",
    "**A8** Replace ??? with code in the code cell below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate classifier with linear kernel and C=1.0\n",
    "linear_svc=?\n",
    "\n",
    "\n",
    "# fit classifier to training set\n",
    "?\n",
    "\n",
    "\n",
    "# make predictions on test set\n",
    "?\n",
    "\n",
    "\n",
    "# compute and print accuracy score\n",
    "print('Model accuracy score with linear kernel and C=1.0 : {0:0.4f}'. format(?))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as pl\n",
    "\n",
    "cm = ?\n",
    "plt.title(\"SVM Model - Confusion Matrix\")\n",
    "?\n",
    "?\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q9** Create a Grid Search for finetuning the value of ```C``` in SVM Model on the ```training set```,\n",
    "\n",
    "``Task:``\n",
    "\n",
    "1. Now, Create a SVM Model with linear kernel and evaluate the model for different values of C. ```Use 'C': [0.01, 0.1, 5, 10, 100]```\n",
    "2. Use the [sklearn GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) method for finetuning the ```linear SVM```.\n",
    "3. Use ```3``` fold of Cross Validation\n",
    "4. Use ```accuracy``` as the scoring technique\n",
    "5. Use ```clf.cv_results_ & clf.best_params_``` for getting the fine-tuned results from the Cross Validation run.\n",
    "5. Now, Plot the Confusion Matrix for test data, using the ```best value of C``` we found from our finetune.  \n",
    "\n",
    "Note: The Grid Search may take couple of minutes. Please wait untill the cell compiles\n",
    "\n",
    "**A9** Replace ??? with code in the code cell below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Select the optimal C parameter by cross-validation\n",
    "tuned_parameters = ?\n",
    "clf =?\n",
    "clf.?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model=?\n",
    "best_model.?\n",
    "cm = ?\n",
    "plt.title(\"SVM Model - Confusion Matrix\")\n",
    "?\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see that after using the Best Value of ```C```, we have less amount of false positive in our test data prediction."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of CS4347-Assignment2-NETID.ipynb",
   "provenance": [
    {
     "file_id": "1vsORsvIGVbJ2cNHR1lIPDIuGobP25ZPq",
     "timestamp": 1629391503766
    },
    {
     "file_id": "1vuQua73YBPg3xOVKXACAGdS_8R1OlQdX",
     "timestamp": 1611597429764
    },
    {
     "file_id": "1Jr8VoifAgTlPqVE_AiCDeWbiHGEyvkxq",
     "timestamp": 1580784119108
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "6e3520c759e5572dcce85a4d2a3287416a08b75cbd9d64f3e9845a08c03ad027"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
